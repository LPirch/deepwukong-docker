{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load config and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukpirch/workspace/deepwukong-docker/.venv/lib/python3.8/site-packages/pytorch_lightning/metrics/__init__.py:43: LightningDeprecationWarning: `pytorch_lightning.metrics.*` module has been renamed to `torchmetrics.*` and split off to its own package (https://github.com/PyTorchLightning/metrics) since v1.3 and will be removed in v1.5\n",
      "  rank_zero_deprecation(\n",
      "Global seed set to 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gnn                                                                                               | classifier         | hyper_parameters            \n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "name: surrogate                                                                                   | hidden_size: 512   | vector_length: 128          \n",
      "w2v_path: data/CWE119/w2v.wv                                                                      | n_hidden_layers: 2 | n_epochs: 50                \n",
      "embed_size: 256                                                                                   | n_classes: 2       | patience: 10                \n",
      "hidden_size: 256                                                                                  | drop_out: 0.5      | batch_size: 64              \n",
      "pooling_ratio: 0.8                                                                                |                    | test_batch_size: 64         \n",
      "drop_out: 0.5                                                                                     |                    | reload_dataloader: True     \n",
      "n_hidden_layers: 5                                                                                |                    | clip_norm: 5                \n",
      "n_head: 3                                                                                         |                    | val_every_step: 1.0         \n",
      "n_gru: 3                                                                                          |                    | log_every_n_steps: 50       \n",
      "edge_sample_ratio: 0.8                                                                            |                    | progress_bar_refresh_rate: 1\n",
      "rnn: {'hidden_size': 256, 'num_layers': 1, 'drop_out': 0.5, 'use_bi': True, 'activation': 'relu'} |                    | resume_from_checkpoint: None\n",
      "                                                                                                  |                    | shuffle_data: True          \n",
      "                                                                                                  |                    | optimizer: Adam             \n",
      "                                                                                                  |                    | nesterov: True              \n",
      "                                                                                                  |                    | learning_rate: 0.002        \n",
      "                                                                                                  |                    | weight_decay: 0             \n",
      "                                                                                                  |                    | decay_gamma: 0.95           \n"
     ]
    }
   ],
   "source": [
    "from typing import cast\n",
    "\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "from commode_utils.common import print_config\n",
    "from pytorch_lightning import seed_everything\n",
    "from src.vocabulary import Vocabulary\n",
    "\n",
    "config_path = 'configs/surrogate.yaml'\n",
    "config = cast(DictConfig, OmegaConf.load(config_path))\n",
    "print_config(config, [\"gnn\", \"classifier\", \"hyper_parameters\"])\n",
    "seed_everything(config.seed, workers=True)\n",
    "\n",
    "vocab = Vocabulary.build_from_w2v(config.gnn.w2v_path)\n",
    "vocab_size = vocab.get_vocab_size()\n",
    "pad_idx = vocab.get_pad_id()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model and DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.models.vd import DeepWuKong\n",
    "from src.datas.datamodules import XFGDataModule\n",
    "\n",
    "checkpoint_path = './results/surrogate-2023-04-03.ckpt'\n",
    "model = DeepWuKong.load_from_checkpoint(checkpoint_path)\n",
    "config = model.hparams[\"config\"]\n",
    "vocabulary = model.hparams[\"vocab\"]\n",
    "data_module = XFGDataModule(config, vocabulary)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset_path = osp.join(config.data_folder, config.dataset.name, 'train.json')\n",
    "dataset = data_module._XFGDataModule__create_dataset(train_dataset_path, retain_source=True)\n",
    "dl = DataLoader(dataset, batch_size=1, collate_fn=data_module.collate_wrapper)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load single sample (vulnerability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = None\n",
    "for sample in dl:\n",
    "    if sample.labels[0] == 1:\n",
    "        x = sample\n",
    "        break\n",
    "x.labels[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true: 1, predicted: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "y_pred = torch.argmax(model(x.graphs))\n",
    "print(f\"true: {x.labels[0]}, predicted: {y_pred}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get source lines of predicted/explained sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:     tracepoint(stonesoup_trace, variable_signed_integral, \"stonesoup_data.before\", stonesoup_data.before, &stonesoup_data.before, \"INITIAL-STATE\");\n",
      "\n",
      "1:     tracepoint(stonesoup_trace, variable_buffer, \"stonesoup_data.buffer\", stonesoup_data.buffer, \"INITIAL-STATE\");\n",
      "\n",
      "2:     tracepoint(stonesoup_trace, variable_signed_integral, \"stonesoup_data.after\", stonesoup_data.after, &stonesoup_data.after, \"INITIAL-STATE\");\n",
      "\n",
      "3:     tracepoint(stonesoup_trace, trace_point, \"CROSSOVER-POINT: BEFORE\");\n",
      "\n",
      "4:     tracepoint(stonesoup_trace, trace_point, \"TRIGGER-POINT: BEFORE\");\n",
      "\n",
      "5:     strcpy(stonesoup_data.buffer, nondivergent_ejectum);\n",
      "\n",
      "6:     tracepoint(stonesoup_trace, variable_buffer, \"stonesoup_data.buffer\", stonesoup_data.buffer, \"CROSSOVER-STATE\");\n",
      "\n",
      "7:     tracepoint(stonesoup_trace, trace_point, \"CROSSOVER-POINT: AFTER\");\n",
      "\n",
      "8:     stonesoup_opt_var = strlen( stonesoup_data.buffer);\n",
      "\n",
      "9:     for (stonesoup_i = 0; stonesoup_i < stonesoup_opt_var; ++stonesoup_i) {\n",
      "\n",
      "10:         stonesoup_data.buffer[stonesoup_i] = stonesoup_toupper(stonesoup_data.buffer[stonesoup_i]);\n",
      "\n",
      "11:         stonesoup_printf(\"%c\",stonesoup_data.after(stonesoup_data.buffer[stonesoup_i]));\n",
      "\n",
      "12:     tracepoint(stonesoup_trace, variable_signed_integral, \"stonesoup_i\", stonesoup_i, &stonesoup_i, \"FINAL-STATE\");\n",
      "\n",
      "13:     tracepoint(stonesoup_trace, variable_buffer, \"stonesoup_data.buffer\", stonesoup_data.buffer, \"FINAL-STATE\");\n",
      "\n",
      "14:     stonesoup_printf(\"\\n\");\n",
      "\n",
      "15:     tracepoint(stonesoup_trace, weakness_end);\n",
      "\n",
      "16: ;\n",
      "\n",
      "17:         if (congroid_smithereens != 0) \n",
      "\n",
      "18:           free(((char *)congroid_smithereens));\n",
      "\n",
      "19: stonesoup_close_printf_context();\n",
      "\n",
      "20:   if (__sync_bool_compare_and_swap(&macintoshes_trite,0,1)) {;\n",
      "\n",
      "21:     if (mkdir(\"/opt/stonesoup/workspace/lockDir\",509U) == 0) {;\n",
      "\n",
      "22:       tracepoint(stonesoup_trace,trace_location,\"/tmp/tmpLnZHRn_ss_testcase/src-rose/src/dfa.c\",\"dfamust\");\n",
      "\n",
      "23:       stonesoup_setup_printf_context();\n",
      "\n",
      "24:       stonesoup_read_taint(&congroid_smithereens,\"1887\",corporatist_provisionary);\n",
      "\n",
      "25:       if (congroid_smithereens != 0) {;\n",
      "\n",
      "26:         nondivergent_ejectum = ((char *)congroid_smithereens);\n",
      "\n",
      "27:     tracepoint(stonesoup_trace, weakness_start, \"CWE120\", \"D\", \"Buffer Copy without Checking Size of Input\");\n",
      "\n",
      "28:     stonesoup_data.before = stonesoup_toupper;\n",
      "\n",
      "29:     for (stonesoup_i = 0; stonesoup_i < 64; stonesoup_i++) {\n",
      "\n",
      "30:         stonesoup_data.buffer[stonesoup_i] = 0;\n",
      "\n",
      "31:     stonesoup_data.after = stonesoup_toupper;\n",
      "\n",
      "32:     tracepoint(stonesoup_trace, variable_signed_integral, \"stonesoup_i\", stonesoup_i, &stonesoup_i, \"INITIAL-STATE\");\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xfg = dataset._XFGDataset__XFGs[x.idx[0]]\n",
    "relevant_lines = list(range(len(xfg._XFG__source)))  # output of LRP: node IDs correspond to source lines\n",
    "for linenum in relevant_lines:\n",
    "    print(f\"{linenum}: {xfg._XFG__source[linenum]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
